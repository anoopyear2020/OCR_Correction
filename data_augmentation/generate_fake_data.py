from nltk import edit_distance
import argparse
import collections
import numpy as np


# This proposed method based on Japanese morphology
def generate_key_groups(source_path, target_path):
    """

    :param source_path: ocr text path generated by OCR model
    :param target_path: ocr ground truth path
    :return: group including key character with candidate list
    """
    key_groups = {}
    with open(source_path, 'r') as f1:
        with open(target_path, 'r') as f2:
            for (sent1, sent2) in zip(f1, f2):
                sent1 = sent1.strip()
                sent2 = sent2.strip()
                if len(sent1) == len(sent2):
                    for i in range(len(sent1)):
                        if sent2[i] not in key_groups:
                            key_groups[sent2[i]] = [sent1[i]]
                        else:
                            key_groups[sent2[i]].append(sent1[i])
                elif len(sent1) - len(sent2) == -1:
                    update_missing_key(sent1, sent2, key_groups)
                # elif len(sent1) - len(sent2) == 1:
                #     update_missing_key(sent2, sent1, key_groups)

    return key_groups


def generate_fake_data(source_path, ocr_out_path, gt_out_path, key_groups):
    """

    :param source_path: path that contains new dataset line by line for generating pairs
    :param ocr_out_path: path that stores ocr output text
    :param gt_out_path: path that stores ground truth text
    :param key_groups: generated by above step
    """
    f_ocr = open(ocr_out_path, 'w')
    f_gt = open(gt_out_path, 'w')

    count = 0
    with open(source_path, 'r') as f:
        for line in f.readlines():
            line = line.strip()
            count += 1

            if count % 10000 == 0:
                print(count)

            new_line = ''
            for i in range(len(line)):
                if line[i] not in key_groups:
                    new_line += line[i]
                else:
                    counter = collections.Counter(key_groups[line[i]])
                    values = list(counter.values())
                    new_vals = [item / sum(values) for item in values]
                    new_ch = np.random.choice(a=list(counter.keys()), size=1, replace=False, p=new_vals)
                    new_line += str(new_ch[0])

            f_ocr.write(new_line + '\n')
            f_gt.write(line + '\n')


def get_proportion(source_path, target_path):
    a = {}
    count = 0
    with open(source_path, 'r') as f1:
        with open(target_path, 'r') as f2:
            for (sent1, sent2) in zip(f1, f2):
                count += 1
                sent1 = sent1.strip()
                sent2 = sent2.strip()
                if (len(sent2) - len(sent1)) not in a:
                    a[len(sent2) - len(sent1)] = 1
                else:
                    a[len(sent2) - len(sent1)] += 1

    for item in a.keys():
        a[item] = a[item] * 1.0 / count

    return a


def update_missing_key(sent_x, sent_y, key_groups):
    """

    :param sent_x: ocr text
    :param sent_y: ground truth text
    :param key_groups:
    """
    if edit_distance(sent_x, sent_y) == 1:
        for item in sent_y:
            if item not in sent_x:
                if item not in key_groups:
                    key_groups[item] = [""]
                else:
                    key_groups[item].append("")


if __name__ == "__main__":
    parser = argparse.ArgumentParser("Generating fake pairs of dataset based on group of similar keys")
    parser.add_argument("--ocr_path", required=True)
    parser.add_argument("--ground_truth_path", required=True)
    parser.add_argument("--source_set_path", required=True)
    parser.add_argument("--ocr_output_path", required=True)
    parser.add_argument("--ground_truth_output_path", required=True)

    args = parser.parse_args()

    # Generate groups of keys
    key_grp = generate_key_groups(args.ocr_path, args.ground_truth_path)

    # Generate pairs of fake dataset
    generate_fake_data(args.source_set_path, args.ocr_output_path, args.ground_truth_output_path, key_grp)

# key_grp = generate_key_groups("/home/brian/Downloads/name_dataset_8/2/x.txt",
#                               "/home/brian/Downloads/name_dataset_8/2/y.txt")
# print(len(key_grp))
# # print(key_grp)
#
# generate_fake_data('/home/brian/Downloads/name_packaged/Frequency/full_names.txt',
#                    '/home/brian/Downloads/name_packaged/Frequency/x.txt',
#                    '/home/brian/Downloads/name_packaged/Frequency/y.txt',
#                    key_grp)
